##############################################################################
# Copyright (c) Members of the EGEE Collaboration. 2004. 
# See http://www.eu-egee.org/partners/ for details on the copyright 
# holders.  
#
# Licensed under the Apache License, Version 2.0 (the "License"); 
# you may not use this file except in compliance with the License. 
# You may obtain a copy of the License at 
#
#    http://www.apache.org/licenses/LICENSE-2.0 
#
# Unless required by applicable law or agreed to in writing, software 
# distributed under the License is distributed on an "AS IS" BASIS, 
# WITHOUT WARRANTIES OR CONDITIONS 
# OF ANY KIND, either express or implied. 
# See the License for the specific language governing permissions and 
# limitations under the License.
##############################################################################
#
# NAME :        site-info.def
#
# DESCRIPTION : This is the main configuration file needed to execute the
#               yaim command. It contains a list of the variables needed to
#               configure a service.
#
# AUTHORS :     yaim-contact@cern.ch
#
# NOTES :       - site-info.def currently contains the whole list of variables
#                 needed to configure a site. However we have started to  move
#                 towards a new approach where node type specific variables will 
#                 be distributed by its corresponding module. 
#                 Although a unique site-info.def can still be used at configuration time.
#               
#               - Service specific variables will be distributed under 
#                 /opt/glite/yaim/examples/siteinfo/services/glite_<node_type_name>
#                 The definition of the variables can be done there or copy them in site-info.def.  
#
#               - VO variables are currently distributed for a number of VOs with
#                 real values that can be directly used by sys admins.
#                 We have started to move towards a new approach where yaim will no longer distribute
#                 these variables. Instead, VO values will be downloaded directly from the CIC
#                 portal and will be integrated using the YAIM configurator.
#
#               - For more information on YAIM, please check:
#                 https://twiki.cern.ch/twiki/bin/view/EGEE/YAIM
#
#               - For more details on the CIC portal, visit:
#                 http://cic.in2p3.fr/
#                 To know more about the YAIM configurator go to the VO management section.
#
# YAIM MODULE:  glite-yaim-core
#                 
##############################################################################

##########################
# YAIM related variables #
##########################

# This a variable to debug your configuration.
# If it is set, functions will print debugging information.
# Values: NONE, ABORT, ERROR, WARNING, INFO, DEBUG
YAIM_LOGGING_LEVEL=INFO

# Repository settings
# Be aware that the install option is only available for 3.0 services.
# You can ignore this variables if you are configuring a 3.1 service.
#LCG_REPOSITORY="'rpm http://glitesoft.cern.ch/EGEE/gLite/APT/R3.0/ rhel30 externals Release3.0 updates'"
#CA_REPOSITORY="rpm http://linuxsoft.cern.ch/ LCG-CAs/current production"
#REPOSITORY_TYPE="apt"

###################################
# General configuration variables #
###################################

MY_DOMAIN=ph.liv.ac.uk
INSTALL_ROOT=/opt

# These variables tell YAIM where to find additional configuration files.
WN_LIST=/opt/glite/yaim/etc/wn-list.conf
USERS_CONF=/opt/glite/yaim/etc/users.conf
GROUPS_CONF=/opt/glite/yaim/etc/groups.conf
FUNCTIONS_DIR=/opt/glite/yaim/functions

# Set this to "yes" if your site provides an X509toKERBEROS Authentication Server
# Only for sites with Experiment Software Area under AFS
GSSKLOG=no
#GSSKLOG_SERVER=my-gssklog.$MY_DOMAIN

OUTPUT_STORAGE=/tmp/jobOutput
JAVA_LOCATION="/usr/java/default/"

# Set this to '/dev/null' or some other dir if you want
# to turn off yaim installation of cron jobs
CRON_DIR=/etc/cron.d

# Set this to your prefered and firewall allowed port range
GLOBUS_TCP_PORT_RANGE="20000,25000"

# Choose a good password !
# And be sure that this file cannot be read by any grid job !
MYSQL_PASSWORD=nufc4EVA

# Site-wide settings
SITE_EMAIL=gridteam@hep.ph.liv.ac.uk
SITE_CRON_EMAIL=$SITE_EMAIL  # not yet used will appear in a later release
SITE_SUPPORT_EMAIL=$SITE_EMAIL
SITE_NAME=UKI-NORTHGRID-LIV-HEP
SITE_LOC="Liverpool, UK"
SITE_LAT=53.4035
SITE_LONG=-2.964
SITE_WEB="http://www.gridpp.ac.uk/northgrid/liverpool"
#SITE_HTTP_PROXY="myproxy.my.domain"
SITE_OTHER_GRID="EGI|WLCG|NORTHGRID|GRIDPP"
SITE_SECURITY_EMAIL="security@hep.ph.liv.ac.uk"
SITE_DESC="University of Liverpool"
SITE_OTHER_BLOG="http://northgrid-tech.blogspot.com/feeds/posts/default"
SITE_OTHER_ICON="http://planet.gridpp.ac.uk/images/northgrid.png"
SITE_OTHER_WLCG_NAMEICON="http://planet.gridpp.ac.uk/images/northgrid.png"
#SITE_OTHER_EGEE_ROC="UK/I"
SITE_OTHER_EGI_NGI="NGI_UK"
SITE_OTHER_EGEE_SERVICE="prod"
SITE_OTHER_WLCG_TIER=2
SITE_OTHER_WLCG_PARENT=RAL-LCG2
SITE_OTHER_WLCG_NAME="UK-Northgrid"

# Set this if your WNs have a shared directory for temporary storage
CE_DATADIR=""

##############################
# CE configuration variables #
##############################

CE_HOST=hepgrid10.$MY_DOMAIN
CE2_HOST=hepgrid6.$MY_DOMAIN

TORQUE_SERVER=hammer.ph.liv.ac.uk

# New variables for CE release 3.1.32
#------------------------------------
# How these values were determined:
#   CE_OTHERDESCR=Cores=1,          <a "typical" number of cores per WN; we have hundreds of single core dells>
#      Benchmark=5.32-HEP-SPEC06    <HEPSPEC-06, per-core basis, averaged over entire cluster>
#                                   <            note that we took the 8 core nodes out?
#   CE_CAPABILITY=
#      CPUScalingReferenceSI00=1330 <converted HEPSPEC06 value for the reference Dell nodes>
#   CE_SI00=1330                    <use same value until APEL is updated>
#
# CREAM_CE_STATE=Production ## Redundant, see services dir
CE_OTHERDESCR=Cores=5.47,Benchmark=11.69-HEP-SPEC06
CE_SI00=2922
CE_CAPABILITY="CPUScalingReferenceSI00=2500 Share=atlas:63 Share=lhcb:25 glexec"

# Totally bogus value based on ratio computed on hg2
CE_SF00=1820
SE_MOUNT_INFO_LIST=none


# Architecture and enviroment specific settings
CE_CPU_MODEL=Xeon
CE_CPU_VENDOR=intel
CE_CPU_SPEED=2400
CE_OS=ScientificSL
CE_OS_RELEASE=5.3
CE_OS_VERSION=Boron
CE_OS_ARCH=x86_64
CE_MINPHYSMEM=16384
CE_MINVIRTMEM=16384
CE_PHYSCPU=174
CE_LOGCPU=952
CE_SMPSIZE=8

CE_OUTBOUNDIP=TRUE
CE_INBOUNDIP=FALSE
CE_RUNTIMEENV="
    HYDRA-CLIENT-3_1
    LCG-2
    LCG-2_1_0
    LCG-2_1_1
    LCG-2_2_0
    LCG-2_3_0
    LCG-2_3_1
    LCG-2_4_0
    LCG-2_5_0
    LCG-2_6_0
    LCG-2_7_0
    GLITE-3_0_0
    GLITE-3_1_0
    R-GMA
"
##############################
# TORQUE/MAUI config vars    #
##############################

# No change to some files
CONFIG_MAUI=no
CONFIG_TORQUE_NODES=no

# Tell APEL to use old location for TORQUE (would default to wrong /var/torque)
TORQUE_VAR_DIR=/var/spool/pbs

##############################
# LB configuration variables #
##############################

LB_HOST=wms105.cern.ch

##############################
# RB configuration variables #
##############################

RB_HOST=lcgrb01.gridpp.rl.ac.uk

###############################
# WMS configuration variables #
###############################

WMS_HOST=grid-wms0.desy.de

###################################
# myproxy configuration variables #
###################################

PX_HOST=lcgrbp01.gridpp.rl.ac.uk

# GRID_TRUSTED_BROKERS: DNs of services (RBs) allowed to renew/retrives
# credentials from/at the myproxy server. Put single quotes around each trusted DN !!!

GRID_TRUSTED_BROKERS="
'broker one'
'broker two'
"

################################
# RGMA configuration variables #
################################

MON_HOST=hepgrid2.$MY_DOMAIN
# REG_HOST=lcgic01.gridpp.rl.ac.uk	

###################################
# FTS configuration variables #
###################################

FTS_HOST=lcgfts.gridpp.rl.ac.uk
FTS_SERVER_URL="https://lcgfts.gridpp.rl.ac.uk:8443/glite-data-transfer-fts"

###############################
# LFC configuration variables #
###############################

#LFC_HOST=my-lfc.$MY_DOMAIN

#LFC_DB_PASSWORD="lfc_password"

# Default value is to put the standard database on the LFC host
#LFC_DB_HOST=$LFC_HOST
#LFC_DB=cns_db

# If you use a DNS alias in front of your LFC, specify it here
#LFC_HOST_ALIAS=""

# All catalogues are local unless you add a VO to
# LFC_CENTRAL, in which case that will be central
#LFC_CENTRAL=""

# If you want to limit the VOs your LFC serves, add the locals here
#LFC_LOCAL=""

#########################################
# Torque server configuration variables #
#########################################

# Change this if your torque server is not on the CE
# This is ignored for other batch systems
BATCH_SERVER=$TORQUE_SERVER

# Jobmanager specific settings
JOB_MANAGER=pbs
CE_BATCH_SYS=pbs
BATCH_BIN_DIR=/usr/bin
BATCH_VERSION=torque-2.1.9
BATCH_LOG_DIR=/var/spool/pbs/server_priv/accounting

#################################
# VOBOX configuration variables #
#################################

#VOBOX_HOST=my-vobox.$MY_DOMAIN
#VOBOX_PORT=1975

################################
# APEL configuration variables #
################################

APEL_DB_PASSWORD="nufc4EVA"
APEL_PUBLISH_USER_DN=yes 

##########################################
# Gridice server configuration variables #
##########################################

# GridIce server host name (usually run on the MON node).
GRIDICE_SERVER_HOST=$MON_HOST

####################################
# E2EMONIT configuration variables #
####################################

# This specifies the location to download the host specific configuration file
E2EMONIT_LOCATION=grid-deployment.web.cern.ch/grid-deployment/e2emonit/production

# Replace this with the siteid supplied by the person setting up the networking
# topology.
E2EMONIT_SITEID=UKI-NORTHGRID-LIV-HEP

######################################
# SE classic configuration variables #
######################################

# Classic SE
#CLASSIC_HOST="classic_SE_host"
#CLASSIC_STORAGE_DIR="/storage"

##################################
# dcache configuration variables #
##################################

# dCache-specific settings
# ignore if you are not running d-cache

# Your dcache admin node
#DCACHE_ADMIN="hepgridXXX5.ph.liv.ac.uk"

# Pools must include host:/absolutePath and may optionally include
# size host:size:/absolutePath if the size is not set the pool will 
# fill the partition it is installed upon. size cannot be smaller 
# than 4 (Gb) unless you are an expert.

#DCACHE_POOLS="hepgrid6.ph.liv.ac.uk/pool"

# Optional

# For large sites the load on the admin-node is a limiting factor. Pnfs
# accounts for a lot of this load and so can be placed on a different
# node to balance the load better.


# Set DCACHE_DOOR_* to "off" if you dont want the door to start on any host
#

#DCACHE_DOOR_SRM="segrid1.ph.liv.ac.uk"
#DCACHE_DOOR_GSIFTP="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_GSIDCAP="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_DCAP="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_XROOTD="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_LDAP="segrid1.ph.liv.ac.uk"
# DCACHE_DOOR_XROOTD="door_node1[:port] door_node2[:port]"

# This option sets the pnfs server it defaults to the admin node if 
# not stated.
#
#DCACHE_PNFS_SERVER="segrid0.ph.liv.ac.uk"
#
# Sets the portrange for dcache as a GSIFTP server in "passive" mode
#
#DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP=50000,52000
#
# Sets the portrange for dcache as a (GSI)DCAP and xrootd server in 
# "passive" mode
#
#DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC=60000,62000
#
# Sets the portrange for dcache as a GSIFTP client in "active" mode
#
#DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP=33115,33215

# Only change if your site has an existing D-Cache installed
# To a different storage root.
#DCACHE_PNFS_VO_DIR="/pnfs/${MY_DOMAIN}/data"

# Set to "yes" only if YAIM shall reset the dCache configuration,
# or install DCache for the first time.
# i.e. if you want YAIM to configure dCache - WARNING:
# this may wipe out any dCache parameters previously configured!

# RESET_DCACHE_CONFIGURATION=no

# Set to "yes" only if YAIM shall reset the dCache nameserver,
# Or install DCache for the first time.
# i.e. if you want YAIM to clear the content of dCache - WARNING:
# this may wipe out any dCache files previously stored!
# RESET_DCACHE_PNFS=no

# Set to "yes" only if YAIM shall reset the dCache Databases,
# or install DCache for the first time.
# i.e. if you want YAIM to clear the metadata of dCache - WARNING:
# this may wipe out any dCache files names previously stored!
# Leaving your system without any way to reestablish which files 
# are stored.
# RESET_DCACHE_RDBMS=no

###############################
# DPM configuration variables #
###############################

# DPMDATA is now deprecated. Use an entry like $DPM_HOST:/filesystem in
# the DPM_FILESYSTEMS variable.
# From now on we use DPM_DB_USER and DPM_DB_PASSWORD to make clear
# its different role from that of the dpmmgr unix user who owns the
# directories and runs the daemons.

# The name of the DPM head node
DPM_HOST="hepgrid11.ph.liv.ac.uk"   # my-dpm.$MY_DOMAIN

# The DPM pool name (max 15 character long name)
DPMPOOL=newPool

# The filesystems/partitions parts of the pool
DPM_FILESYSTEMS="/data"

# The database user
DPM_DB_USER=dpmuser

# The database user password
DPM_DB_PASSWORD=nufc4EVA

# The DPM database host
DPM_DB_HOST=$DPM_HOST

# The DPM db name. Default is dpm_db
DPM_DB=dpm_db

# The DPNS db name. Default is cns_db
DPNS_DB=cns_db

# The DPM infosystem user name
DPM_INFO_USER=dpminfo

# The DPM infosystem user password
DPM_INFO_PASS=nufc4EVA

# Specifies the default amount of space reserved  for a file
DPMFSIZE=1G

# Variable for the port range  - Optional, default value is shown
RFIO_PORT_RANGE="20000 25000"

DPM_HTTPS="no"

DPM_XROOTD="no"

SE_GRIDFTP_LOGFILE=/var/log/dpm-gsiftp/dpm-gsiftp.log

# Take defintions from users.conf and place here for 1.8.2
DPMMGR_USER=dpmmgr
DPMMGR_GROUP=dpmmgr
DPMMGR_UID=104
DPMMGR_GID=104
DPMMGR_USER_HOME=/home/dpmmgr

###########
# SE_LIST #
###########

SE_LIST="$DPM_HOST"
SE_ARCH="multidisk" # "disk, tape, multidisk, other"

################################
# BDII configuration variables #
################################

BDII_HOST=lcg-bdii.gridpp.ac.uk
BDII_LIST=lcg-bdii.gridpp.ac.uk:2170,topbdii.grid.hep.ph.ic.ac.uk:2170
SITE_BDII_HOST=hepgrid4.$MY_DOMAIN
BDII_DELETE_DELAY=0

BDII_SITE_TIMEOUT=120
BDII_RESOURCE_TIMEOUT=`expr "$BDII_SITE_TIMEOUT" - 5`
GIP_RESPONSE=`expr "$BDII_RESOURCE_TIMEOUT" - 5`
GIP_FRESHNESS=60
GIP_CACHE_TTL=300
GIP_TIMEOUT=150

# Check the validity of this URL in the documentation
BDII_HTTP_URL="http://grid-deployment.web.cern.ch/grid-deployment/gis/lcg2-bdii/dteam/lcg2-all-sites.conf"

# The Freedom of Choice of Resources service allows a top-level BDII
# to be instructed to remove VO-specific access control lines for
# resources that do not meet the VO requirements
BDII_FCR=http://lcg-fcr.cern.ch:8083/fcr-data/exclude.ldif

BDII_REGIONS="CE CE2 SE SITE_BDII"    # list of the services provided by the site

BDII_CE_URL="ldap://$CE_HOST:2170/mds-vo-name=resource,o=grid"
BDII_CE2_URL="ldap://$CE2_HOST:2170/mds-vo-name=resource,o=grid"
BDII_SE_URL="ldap://$DPM_HOST:2170/mds-vo-name=resource,o=grid"
BDII_SITE_BDII_URL="ldap://$SITE_BDII_HOST:2170/mds-vo-name=resource,o=grid"

##############################
# VO configuration variables #
##############################
#
# Edit the following set of variables if you want to configure a different VO:
# VO_<vo_name>_SW_DIR
# VO_<vo_name>_DEFAULT_SE
# VO_<vo_name>_STORAGE_DIR 
# VO_<vo_name>_VOMS_SERVERS
# VO_<vo_name>_VOMS_POOL_PATH  (optional)
# VO_<vo_name>_VOMS_EXTRA_MAPS (optional)
# VO_<vo_name>_VOMSES
# VO_<vo_name>_VOMS_CA_DN
# 
# If you are configuring a DNS-like VO, please check
# the following URL: https://twiki.cern.ch/twiki/bin/view/LCG/YaimGuide400#vo_d_directory
#
# IMPORTANT! Please, take into account that in the future YAIM will no longer provide VO
# related variables for these VOs. This information should be obtained out of the CIC portal:
# http://cic.in2p3.fr/
#
# The VO variables will be automatically generated by the YAIM configurator and integrated in YAIM. 

# Space separated list of supported VOs by your site

QUEUES="long"

VOS="alice atlas babar biomed calice camont cdf cedar cms dteam dzero esr fusion geant4 hone gridpp ilc lhcb ltwo magic mice minos.vo.gridpp.ac.uk na48 ngs.ac.uk na62.vo.gridpp.ac.uk ops pheno planck vo.sixt.cern.ch snoplus.snolab.ca supernemo.vo.eu-egee.org t2k.org totalep vo.northgrid.ac.uk zeus"

LONG_GROUP_ENABLE="\
 alice /alice/ROLE=lcgadmin /alice/ROLE=production\
 atlas /atlas/ROLE=lcgadmin /atlas/ROLE=production /atlas/ROLE=pilot\
 babar /babar/ROLE=lcgadmin\
 biomed /biomed/ROLE=lcgadmin\
 calice /calice/ROLE=lcgadmin /calice/ROLE=production\
 camont /camont/ROLE=lcgadmin\
 cdf /cdf/ROLE=lcgadmin\
 cedar /cedar/ROLE=lcgadmin\
 cms /cms/ROLE=lcgadmin /cms/ROLE=production\
 dteam /dteam/ROLE=lcgadmin /dteam/ROLE=production\
 dzero /dzero/ROLE=lcgadmin\
 esr /esr/ROLE=lcgadmin\
 fusion /fusion/ROLE=production\
 geant4 /geant4/ROLE=lcgadmin /geant4/ROLE=production\
 gridpp /gridpp/ROLE=lcgadmin\
 hone /hone/ROLE=lcgadmin /hone/ROLE=production\
 ilc /ilc/ROLE=lcgadmin /ilc/ROLE=production\
 lhcb /lhcb/ROLE=lcgadmin /lhcb/ROLE=production /lhcb/ROLE=pilot\
 ltwo /ltwo/ROLE=lcgadmin\
 magic /magic/ROLE=lcgadmin\
 mice /mice/ROLE=lcgadmin /mice/ROLE=production\
 minos.vo.gridpp.ac.uk /minos.vo.gridpp.ac.uk/ROLE=lcgadmin\
 na48 /na48/ROLE=lcgadmin\
 na62.vo.gridpp.ac.uk /na62.vo.gridpp.ac.uk/ROLE=lcgadmin /na62.vo.gridpp.ac.uk/ROLE=production\
 ngs.ac.uk /ngs.ac.uk/ROLE=lcgadmin\
 vo.northgrid.ac.uk /vo.northgrid.ac.uk/ROLE=lcgadmin\
 ops /ops/ROLE=lcgadmin /ops/ROLE=pilot\
 pheno /pheno/ROLE=lcgadmin\
 planck /planck/ROLE=lcgadmin /planck/ROLE=production\
 vo.sixt.cern.ch /vo.sixt.cern.ch/ROLE=lcgadmin\
 snoplus.snolab.ca /snoplus.snolab.ca/ROLE=lcgadmin /snoplus.snolab.ca/ROLE=production\
 supernemo.vo.eu-egee.org /supernemo.vo.eu-egee.org/ROLE=lcgadmin /supernemo.vo.eu-egee.org/ROLE=production\
 t2k.org /t2k.org/ROLE=lcgadmin /t2k.org/ROLE=production\
 totalep /totalep/Role=lcgadmin /totalep/Role=production\
 zeus /zeus/ROLE=lcgadmin /zeus/ROLE=production"

# Global VO varialbes
VO_SW_DIR=/opt/exp_soft_sl5/
STORAGE_PATH=/dpm/$MY_DOMAIN/home

# Set this if you want a scratch directory for jobs
EDG_WL_SCRATCH=""

# ALICE

# ATLAS


# BIOMED


# CMS

##--------------------------
# DTEAM SWAPPED TO GREECE
##--------------------------





# DTEAM AS WAS
#VO_DTEAM_VOMS_SERVERS="'vomss://lcg-voms.cern.ch:8443/voms/dteam?/dteam' 'vomss://voms.cern.ch:8443/voms/dteam?/dteam'"
#VO_DTEAM_VOMSES="'dteam lcg-voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch dteam' 'dteam voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch dteam'"
#VO_DTEAM_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"

# ESR

# GEANT4

# LHCB


# MAGIC

# PLANCK

# OPS

#VO_OPS_VOMS_SERVERS="vomss://voms.cern.ch:8443/voms/ops?/ops/"
#VO_OPS_VOMSES="'ops voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch ops'"
#VO_OPS_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"


# BABAR

# CDF

# DZERO


# HONE

# ILC


# NA48

# ZEUS

# GRIDPP

# PHENO

# CEDAR

# MICE

# LTWO

# TOTALEP

# FUSION

# CALICE

# CAMONT

# MINOS
# see vo.d/minos.vo.gridpp.ac.uk
# SIXT
# see vo.d/vo.sixt.cern.ch

# SUPERNEMO
# see vo.d/supernemo.vo.eu-egee.org

# NGS
# see vo.d/ngs.ac.uk

# NORTHGRID
# see vo.d/vo.northgrid.ac.uk


