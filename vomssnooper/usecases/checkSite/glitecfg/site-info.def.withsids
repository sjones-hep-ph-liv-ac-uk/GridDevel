##############################################################################
# Copyright (c) Members of the EGEE Collaboration. 2004. 
# See http://www.eu-egee.org/partners/ for details on the copyright 
# holders.  
#
# Licensed under the Apache License, Version 2.0 (the "License"); 
# you may not use this file except in compliance with the License. 
# You may obtain a copy of the License at 
#
#    http://www.apache.org/licenses/LICENSE-2.0 
#
# Unless required by applicable law or agreed to in writing, software 
# distributed under the License is distributed on an "AS IS" BASIS, 
# WITHOUT WARRANTIES OR CONDITIONS 
# OF ANY KIND, either express or implied. 
# See the License for the specific language governing permissions and 
# limitations under the License.
##############################################################################
#
# NAME :        site-info.def
#
# DESCRIPTION : This is the main configuration file needed to execute the
#               yaim command. It contains a list of the variables needed to
#               configure a service.
#
# AUTHORS :     yaim-contact@cern.ch
#
# NOTES :       - site-info.def currently contains the whole list of variables
#                 needed to configure a site. However we have started to  move
#                 towards a new approach where node type specific variables will 
#                 be distributed by its corresponding module. 
#                 Although a unique site-info.def can still be used at configuration time.
#               
#               - Service specific variables will be distributed under 
#                 /opt/glite/yaim/examples/siteinfo/services/glite_<node_type_name>
#                 The definition of the variables can be done there or copy them in site-info.def.  
#
#               - VO variables are currently distributed for a number of VOs with
#                 real values that can be directly used by sys admins.
#                 We have started to move towards a new approach where yaim will no longer distribute
#                 these variables. Instead, VO values will be downloaded directly from the CIC
#                 portal and will be integrated using the YAIM configurator.
#
#               - For more information on YAIM, please check:
#                 https://twiki.cern.ch/twiki/bin/view/EGEE/YAIM
#
#               - For more details on the CIC portal, visit:
#                 http://cic.in2p3.fr/
#                 To know more about the YAIM configurator go to the VO management section.
#
# YAIM MODULE:  glite-yaim-core
#                 
##############################################################################

##########################
# YAIM related variables #
##########################

# This a variable to debug your configuration.
# If it is set, functions will print debugging information.
# Values: NONE, ABORT, ERROR, WARNING, INFO, DEBUG
YAIM_LOGGING_LEVEL=INFO

# Repository settings
# Be aware that the install option is only available for 3.0 services.
# You can ignore this variables if you are configuring a 3.1 service.
#LCG_REPOSITORY="'rpm http://glitesoft.cern.ch/EGEE/gLite/APT/R3.0/ rhel30 externals Release3.0 updates'"
#CA_REPOSITORY="rpm http://linuxsoft.cern.ch/ LCG-CAs/current production"
#REPOSITORY_TYPE="apt"

###################################
# General configuration variables #
###################################

MY_DOMAIN=ph.liv.ac.uk
INSTALL_ROOT=/opt

# These variables tell YAIM where to find additional configuration files.
WN_LIST=/opt/glite/yaim/etc/wn-list.conf
USERS_CONF=/opt/glite/yaim/etc/users.conf
GROUPS_CONF=/opt/glite/yaim/etc/groups.conf
FUNCTIONS_DIR=/opt/glite/yaim/functions

# Set this to "yes" if your site provides an X509toKERBEROS Authentication Server
# Only for sites with Experiment Software Area under AFS
GSSKLOG=no
#GSSKLOG_SERVER=my-gssklog.$MY_DOMAIN

OUTPUT_STORAGE=/tmp/jobOutput
JAVA_LOCATION="/usr/java/default/"

# Set this to '/dev/null' or some other dir if you want
# to turn off yaim installation of cron jobs
CRON_DIR=/etc/cron.d

# Set this to your prefered and firewall allowed port range
GLOBUS_TCP_PORT_RANGE="20000,25000"

# Choose a good password !
# And be sure that this file cannot be read by any grid job !
MYSQL_PASSWORD=nufc4EVA

# Site-wide settings
SITE_EMAIL=gridteam@hep.ph.liv.ac.uk
SITE_CRON_EMAIL=$SITE_EMAIL  # not yet used will appear in a later release
SITE_SUPPORT_EMAIL=$SITE_EMAIL
SITE_NAME=UKI-NORTHGRID-LIV-HEP
SITE_LOC="Liverpool, UK"
SITE_LAT=53.4035
SITE_LONG=-2.964
SITE_WEB="http://www.gridpp.ac.uk/northgrid/liverpool"
#SITE_HTTP_PROXY="myproxy.my.domain"
SITE_OTHER_GRID="EGI|WLCG|NORTHGRID|GRIDPP"
SITE_SECURITY_EMAIL="security@hep.ph.liv.ac.uk"
SITE_DESC="University of Liverpool"
SITE_OTHER_BLOG="http://northgrid-tech.blogspot.com/feeds/posts/default"
SITE_OTHER_ICON="http://planet.gridpp.ac.uk/images/northgrid.png"
SITE_OTHER_WLCG_NAMEICON="http://planet.gridpp.ac.uk/images/northgrid.png"
#SITE_OTHER_EGEE_ROC="UK/I"
SITE_OTHER_EGI_NGI="NGI_UK"
SITE_OTHER_EGEE_SERVICE="prod"
SITE_OTHER_WLCG_TIER=2
SITE_OTHER_WLCG_PARENT=RAL-LCG2
SITE_OTHER_WLCG_NAME="UK-Northgrid"

# Set this if your WNs have a shared directory for temporary storage
CE_DATADIR=""

##############################
# CE configuration variables #
##############################

CE_HOST=hepgrid6.$MY_DOMAIN
CE2_HOST=hepgrid10.$MY_DOMAIN

TORQUE_SERVER=hammer.ph.liv.ac.uk

# New variables for CE release 3.1.32
#------------------------------------
# How these values were determined:
#   CE_OTHERDESCR=Cores=1,          <a "typical" number of cores per WN; we have hundreds of single core dells>
#      Benchmark=5.32-HEP-SPEC06    <HEPSPEC-06, per-core basis, averaged over entire cluster>
#                                   <            note that we took the 8 core nodes out?
#   CE_CAPABILITY=
#      CPUScalingReferenceSI00=1330 <converted HEPSPEC06 value for the reference Dell nodes>
#   CE_SI00=1330                    <use same value until APEL is updated>
#
# CREAM_CE_STATE=Production ## Redundant, see services dir
CE_OTHERDESCR=Cores=5.47,Benchmark=11.69-HEP-SPEC06
CE_SI00=2922
CE_CAPABILITY="CPUScalingReferenceSI00=2500 Share=atlas:63 Share=lhcb:25 glexec"

# Totally bogus value based on ratio computed on hg2
CE_SF00=1820
SE_MOUNT_INFO_LIST=none


# Architecture and enviroment specific settings
CE_CPU_MODEL=Xeon
CE_CPU_VENDOR=intel
CE_CPU_SPEED=2400
CE_OS=ScientificSL
CE_OS_RELEASE=5.3
CE_OS_VERSION=Boron
CE_OS_ARCH=x86_64
CE_MINPHYSMEM=16384
CE_MINVIRTMEM=16384
CE_PHYSCPU=0  
CE_LOGCPU=0  
CE_SMPSIZE=8

CE_OUTBOUNDIP=TRUE
CE_INBOUNDIP=FALSE
CE_RUNTIMEENV="
    HYDRA-CLIENT-3_1
    LCG-2
    LCG-2_1_0
    LCG-2_1_1
    LCG-2_2_0
    LCG-2_3_0
    LCG-2_3_1
    LCG-2_4_0
    LCG-2_5_0
    LCG-2_6_0
    LCG-2_7_0
    GLITE-3_0_0
    GLITE-3_1_0
    R-GMA
"
##############################
# TORQUE/MAUI config vars    #
##############################

# No change to some files
CONFIG_MAUI=no
CONFIG_TORQUE_NODES=no

# Tell APEL to use old location for TORQUE (would default to wrong /var/torque)
TORQUE_VAR_DIR=/var/spool/pbs

##############################
# LB configuration variables #
##############################

LB_HOST=wms105.cern.ch

##############################
# RB configuration variables #
##############################

RB_HOST=lcgrb01.gridpp.rl.ac.uk

###############################
# WMS configuration variables #
###############################

WMS_HOST=grid-wms0.desy.de

###################################
# myproxy configuration variables #
###################################

PX_HOST=lcgrbp01.gridpp.rl.ac.uk

# GRID_TRUSTED_BROKERS: DNs of services (RBs) allowed to renew/retrives
# credentials from/at the myproxy server. Put single quotes around each trusted DN !!!

GRID_TRUSTED_BROKERS="
'broker one'
'broker two'
"

################################
# RGMA configuration variables #
################################

MON_HOST=hepgrid2.$MY_DOMAIN
# REG_HOST=lcgic01.gridpp.rl.ac.uk	

###################################
# FTS configuration variables #
###################################

FTS_HOST=lcgfts.gridpp.rl.ac.uk
FTS_SERVER_URL="https://lcgfts.gridpp.rl.ac.uk:8443/glite-data-transfer-fts"

###############################
# LFC configuration variables #
###############################

#LFC_HOST=my-lfc.$MY_DOMAIN

#LFC_DB_PASSWORD="lfc_password"

# Default value is to put the standard database on the LFC host
#LFC_DB_HOST=$LFC_HOST
#LFC_DB=cns_db

# If you use a DNS alias in front of your LFC, specify it here
#LFC_HOST_ALIAS=""

# All catalogues are local unless you add a VO to
# LFC_CENTRAL, in which case that will be central
#LFC_CENTRAL=""

# If you want to limit the VOs your LFC serves, add the locals here
#LFC_LOCAL=""

#########################################
# Torque server configuration variables #
#########################################

# Change this if your torque server is not on the CE
# This is ignored for other batch systems
BATCH_SERVER=$TORQUE_SERVER

# Jobmanager specific settings
JOB_MANAGER=pbs
CE_BATCH_SYS=pbs
BATCH_BIN_DIR=/usr/bin
BATCH_VERSION=torque-2.1.9
BATCH_LOG_DIR=/var/spool/pbs/server_priv/accounting

#################################
# VOBOX configuration variables #
#################################

#VOBOX_HOST=my-vobox.$MY_DOMAIN
#VOBOX_PORT=1975

################################
# APEL configuration variables #
################################

APEL_DB_PASSWORD="nufc4EVA"
APEL_PUBLISH_USER_DN=yes 

##########################################
# Gridice server configuration variables #
##########################################

# GridIce server host name (usually run on the MON node).
GRIDICE_SERVER_HOST=$MON_HOST

####################################
# E2EMONIT configuration variables #
####################################

# This specifies the location to download the host specific configuration file
E2EMONIT_LOCATION=grid-deployment.web.cern.ch/grid-deployment/e2emonit/production

# Replace this with the siteid supplied by the person setting up the networking
# topology.
E2EMONIT_SITEID=UKI-NORTHGRID-LIV-HEP

######################################
# SE classic configuration variables #
######################################

# Classic SE
#CLASSIC_HOST="classic_SE_host"
#CLASSIC_STORAGE_DIR="/storage"

##################################
# dcache configuration variables #
##################################

# dCache-specific settings
# ignore if you are not running d-cache

# Your dcache admin node
#DCACHE_ADMIN="hepgridXXX5.ph.liv.ac.uk"

# Pools must include host:/absolutePath and may optionally include
# size host:size:/absolutePath if the size is not set the pool will 
# fill the partition it is installed upon. size cannot be smaller 
# than 4 (Gb) unless you are an expert.

#DCACHE_POOLS="hepgrid6.ph.liv.ac.uk/pool"

# Optional

# For large sites the load on the admin-node is a limiting factor. Pnfs
# accounts for a lot of this load and so can be placed on a different
# node to balance the load better.


# Set DCACHE_DOOR_* to "off" if you dont want the door to start on any host
#

#DCACHE_DOOR_SRM="segrid1.ph.liv.ac.uk"
#DCACHE_DOOR_GSIFTP="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_GSIDCAP="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_DCAP="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_XROOTD="segrid2.ph.liv.ac.uk"
#DCACHE_DOOR_LDAP="segrid1.ph.liv.ac.uk"
# DCACHE_DOOR_XROOTD="door_node1[:port] door_node2[:port]"

# This option sets the pnfs server it defaults to the admin node if 
# not stated.
#
#DCACHE_PNFS_SERVER="segrid0.ph.liv.ac.uk"
#
# Sets the portrange for dcache as a GSIFTP server in "passive" mode
#
#DCACHE_PORT_RANGE_PROTOCOLS_SERVER_GSIFTP=50000,52000
#
# Sets the portrange for dcache as a (GSI)DCAP and xrootd server in 
# "passive" mode
#
#DCACHE_PORT_RANGE_PROTOCOLS_SERVER_MISC=60000,62000
#
# Sets the portrange for dcache as a GSIFTP client in "active" mode
#
#DCACHE_PORT_RANGE_PROTOCOLS_CLIENT_GSIFTP=33115,33215

# Only change if your site has an existing D-Cache installed
# To a different storage root.
#DCACHE_PNFS_VO_DIR="/pnfs/${MY_DOMAIN}/data"

# Set to "yes" only if YAIM shall reset the dCache configuration,
# or install DCache for the first time.
# i.e. if you want YAIM to configure dCache - WARNING:
# this may wipe out any dCache parameters previously configured!

# RESET_DCACHE_CONFIGURATION=no

# Set to "yes" only if YAIM shall reset the dCache nameserver,
# Or install DCache for the first time.
# i.e. if you want YAIM to clear the content of dCache - WARNING:
# this may wipe out any dCache files previously stored!
# RESET_DCACHE_PNFS=no

# Set to "yes" only if YAIM shall reset the dCache Databases,
# or install DCache for the first time.
# i.e. if you want YAIM to clear the metadata of dCache - WARNING:
# this may wipe out any dCache files names previously stored!
# Leaving your system without any way to reestablish which files 
# are stored.
# RESET_DCACHE_RDBMS=no

###############################
# DPM configuration variables #
###############################

# DPMDATA is now deprecated. Use an entry like $DPM_HOST:/filesystem in
# the DPM_FILESYSTEMS variable.
# From now on we use DPM_DB_USER and DPM_DB_PASSWORD to make clear
# its different role from that of the dpmmgr unix user who owns the
# directories and runs the daemons.

# The name of the DPM head node
DPM_HOST="hepgrid11.ph.liv.ac.uk"   # my-dpm.$MY_DOMAIN

# The DPM pool name (max 15 character long name)
DPMPOOL=newPool

# The filesystems/partitions parts of the pool
DPM_FILESYSTEMS="/data"

# The database user
DPM_DB_USER=dpmuser

# The database user password
DPM_DB_PASSWORD=nufc4EVA

# The DPM database host
DPM_DB_HOST=$DPM_HOST

# The DPM db name. Default is dpm_db
DPM_DB=dpm_db

# The DPNS db name. Default is cns_db
DPNS_DB=cns_db

# The DPM infosystem user name
DPM_INFO_USER=dpminfo

# The DPM infosystem user password
DPM_INFO_PASS=nufc4EVA

# Specifies the default amount of space reserved  for a file
DPMFSIZE=1G

# Variable for the port range  - Optional, default value is shown
RFIO_PORT_RANGE="20000 25000"

DPM_HTTPS="no"

DPM_XROOTD="no"

SE_GRIDFTP_LOGFILE=/var/log/dpm-gsiftp/dpm-gsiftp.log

# Take defintions from users.conf and place here for 1.8.2
DPMMGR_USER=dpmmgr
DPMMGR_GROUP=dpmmgr
DPMMGR_UID=104
DPMMGR_GID=104
DPMMGR_USER_HOME=/home/dpmmgr

###########
# SE_LIST #
###########

SE_LIST="$DPM_HOST"
SE_ARCH="multidisk" # "disk, tape, multidisk, other"

################################
# BDII configuration variables #
################################

BDII_HOST=lcg-bdii.gridpp.ac.uk
BDII_LIST=lcg-bdii.gridpp.ac.uk:2170,topbdii.grid.hep.ph.ic.ac.uk:2170
SITE_BDII_HOST=hepgrid4.$MY_DOMAIN
BDII_DELETE_DELAY=0

BDII_SITE_TIMEOUT=120
BDII_RESOURCE_TIMEOUT=`expr "$BDII_SITE_TIMEOUT" - 5`
GIP_RESPONSE=`expr "$BDII_RESOURCE_TIMEOUT" - 5`
GIP_FRESHNESS=60
GIP_CACHE_TTL=300
GIP_TIMEOUT=150

# Check the validity of this URL in the documentation
BDII_HTTP_URL="http://grid-deployment.web.cern.ch/grid-deployment/gis/lcg2-bdii/dteam/lcg2-all-sites.conf"

# The Freedom of Choice of Resources service allows a top-level BDII
# to be instructed to remove VO-specific access control lines for
# resources that do not meet the VO requirements
BDII_FCR=http://lcg-fcr.cern.ch:8083/fcr-data/exclude.ldif

BDII_REGIONS="CE CE2 SE SITE_BDII"    # list of the services provided by the site

BDII_CE_URL="ldap://$CE_HOST:2170/mds-vo-name=resource,o=grid"
BDII_CE2_URL="ldap://$CE2_HOST:2170/mds-vo-name=resource,o=grid"
BDII_SE_URL="ldap://$DPM_HOST:2170/mds-vo-name=resource,o=grid"
BDII_SITE_BDII_URL="ldap://$SITE_BDII_HOST:2170/mds-vo-name=resource,o=grid"

##############################
# VO configuration variables #
##############################
#
# Edit the following set of variables if you want to configure a different VO:
# VO_<vo_name>_SW_DIR
# VO_<vo_name>_DEFAULT_SE
# VO_<vo_name>_STORAGE_DIR 
# VO_<vo_name>_VOMS_SERVERS
# VO_<vo_name>_VOMS_POOL_PATH  (optional)
# VO_<vo_name>_VOMS_EXTRA_MAPS (optional)
# VO_<vo_name>_VOMSES
# VO_<vo_name>_VOMS_CA_DN
# 
# If you are configuring a DNS-like VO, please check
# the following URL: https://twiki.cern.ch/twiki/bin/view/LCG/YaimGuide400#vo_d_directory
#
# IMPORTANT! Please, take into account that in the future YAIM will no longer provide VO
# related variables for these VOs. This information should be obtained out of the CIC portal:
# http://cic.in2p3.fr/
#
# The VO variables will be automatically generated by the YAIM configurator and integrated in YAIM. 

# Space separated list of supported VOs by your site

QUEUES="long"

VOS="alice atlas babar biomed calice camont cdf cedar cms dteam dzero esr fusion geant4 hone gridpp ilc lhcb magic mice minos.vo.gridpp.ac.uk na48 ngs.ac.uk na62.vo.gridpp.ac.uk ops pheno planck vo.sixt.cern.ch snoplus.snolab.ca supernemo.vo.eu-egee.org t2k.org vo.northgrid.ac.uk zeus"

LONG_GROUP_ENABLE="\
 alice /alice/ROLE=lcgadmin /alice/ROLE=production\
 atlas /atlas/ROLE=lcgadmin /atlas/ROLE=production /atlas/ROLE=pilot\
 babar /babar/ROLE=lcgadmin\
 biomed /biomed/ROLE=lcgadmin\
 calice /calice/ROLE=lcgadmin /calice/ROLE=production\
 camont /camont/ROLE=lcgadmin\
 cdf /cdf/ROLE=lcgadmin\
 cedar /cedar/ROLE=lcgadmin\
 cms /cms/ROLE=lcgadmin /cms/ROLE=production\
 dteam /dteam/ROLE=lcgadmin /dteam/ROLE=production\
 dzero /dzero/ROLE=lcgadmin\
 esr /esr/ROLE=lcgadmin\
 fusion /fusion/ROLE=production\
 geant4 /geant4/ROLE=lcgadmin /geant4/ROLE=production\
 gridpp /gridpp/ROLE=lcgadmin\
 hone /hone/ROLE=lcgadmin /hone/ROLE=production\
 ilc /ilc/ROLE=lcgadmin /ilc/ROLE=production\
 lhcb /lhcb/ROLE=lcgadmin /lhcb/ROLE=production /lhcb/ROLE=pilot\
 magic /magic/ROLE=lcgadmin\
 mice /mice/ROLE=lcgadmin /mice/ROLE=production\
 minos.vo.gridpp.ac.uk /minos.vo.gridpp.ac.uk/ROLE=lcgadmin\
 na48 /na48/ROLE=lcgadmin\
 na62.vo.gridpp.ac.uk /na62.vo.gridpp.ac.uk/ROLE=lcgadmin /na62.vo.gridpp.ac.uk/ROLE=production\
 ngs.ac.uk /ngs.ac.uk/ROLE=lcgadmin\
 vo.northgrid.ac.uk /vo.northgrid.ac.uk/ROLE=lcgadmin\
 ops /ops/ROLE=lcgadmin /ops/ROLE=pilot\
 pheno /pheno/ROLE=lcgadmin\
 planck /planck/ROLE=lcgadmin /planck/ROLE=production\
 vo.sixt.cern.ch /vo.sixt.cern.ch/ROLE=lcgadmin\
 snoplus.snolab.ca /snoplus.snolab.ca/ROLE=lcgadmin /snoplus.snolab.ca/ROLE=production\
 supernemo.vo.eu-egee.org /supernemo.vo.eu-egee.org/ROLE=lcgadmin /supernemo.vo.eu-egee.org/ROLE=production\
 t2k.org /t2k.org/ROLE=lcgadmin /t2k.org/ROLE=production\
 zeus /zeus/ROLE=lcgadmin /zeus/ROLE=production"

# Global VO varialbes
VO_SW_DIR=/opt/exp_soft_sl5/
STORAGE_PATH=/dpm/$MY_DOMAIN/home

# Set this if you want a scratch directory for jobs
EDG_WL_SCRATCH=""

# ALICE
VO_ALICE_SW_DIR=$VO_SW_DIR/alice
VO_ALICE_DEFAULT_SE=$DPM_HOST
VO_ALICE_STORAGE_DIR=$STORAGE_PATH/alice
VO_ALICE_VOMS_SERVERS="'vomss://voms.cern.ch:8443/voms/alice?/alice' "
VO_ALICE_VOMS_POOL_PATH=""
VO_ALICE_VOMS_EXTRA_MAPS=""
VO_ALICE_VOMSES="'alice voms.cern.ch 15000 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch alice' "
VO_ALICE_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' "

# ATLAS
VO_ATLAS_SW_DIR=/cvmfs/atlas.cern.ch/repo/sw
VO_ATLAS_DEFAULT_SE=$DPM_HOST
VO_ATLAS_STORAGE_DIR=$STORAGE_PATH/atlas
VO_ATLAS_VOMS_SERVERS="'vomss://lcg-voms.cern.ch:8443/voms/atlas?/atlas' 'vomss://voms.cern.ch:8443/voms/atlas?/atlas' 'vomss://vo.racf.bnl.gov:8443/voms/atlas?/atlas' "
VO_ATLAS_VOMS_POOL_PATH="/lcg1"
VO_ATLAS_VOMSES="'atlas lcg-voms.cern.ch 15001 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch atlas' 'atlas voms.cern.ch 15001 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch atlas' 'atlas vo.racf.bnl.gov 15003 /DC=org/DC=doegrids/OU=Services/CN=vo.racf.bnl.gov atlas' "
VO_ATLAS_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1' "


# BIOMED
VO_BIOMED_SW_DIR=$VO_SW_DIR/biomed
VO_BIOMED_DEFAULT_SE=$DPM_HOST
VO_BIOMED_STORAGE_DIR=$STORAGE_PATH/biomed
VO_BIOMED_VOMS_SERVERS="'vomss://cclcgvomsli01.in2p3.fr:8443/voms/biomed?/biomed' "
VO_BIOMED_VOMSES="'biomed cclcgvomsli01.in2p3.fr 15000 /O=GRID-FR/C=FR/O=CNRS/OU=CC-IN2P3/CN=cclcgvomsli01.in2p3.fr biomed' "
VO_BIOMED_VOMS_CA_DN="'/C=FR/O=CNRS/CN=GRID2-FR' "


# CMS
VO_CMS_SW_DIR=$VO_SW_DIR/cms
VO_CMS_DEFAULT_SE=$DPM_HOST
VO_CMS_STORAGE_DIR=$STORAGE_PATH/cms
VO_CMS_VOMS_SERVERS="'vomss://lcg-voms.cern.ch:8443/voms/cms?/cms' 'vomss://voms.cern.ch:8443/voms/cms?/cms' 'vomss://voms.fnal.gov:8443/voms/cms?/cms' "
VO_CMS_VOMS_POOL_PATH=""
VO_CMS_VOMS_EXTRA_MAPS=""
VO_CMS_VOMSES="'cms lcg-voms.cern.ch 15002 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch cms' 'cms voms.cern.ch 15002 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch cms' 'cms voms.fnal.gov 15015 /DC=org/DC=doegrids/OU=Services/CN=http/voms.fnal.gov cms' "
VO_CMS_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1' "

##--------------------------
# DTEAM SWAPPED TO GREECE
##--------------------------

VO_DTEAM_VOMS_SERVERS="'vomss://voms.hellasgrid.gr:8443/voms/dteam?/dteam' 'vomss://voms2.hellasgrid.gr:8443/voms/dteam?/dteam' "

VO_DTEAM_VOMSES="'dteam voms.hellasgrid.gr 15004 /C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms.hellasgrid.gr dteam' 'dteam voms2.hellasgrid.gr 15004 /C=GR/O=HellasGrid/OU=hellasgrid.gr/CN=voms2.hellasgrid.gr dteam' "


VO_DTEAM_VOMS_CA_DN="'/C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid CA 2006' '/C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid CA 2006' "

# DTEAM AS WAS
VO_DTEAM_SW_DIR=$VO_SW_DIR/dteam
VO_DTEAM_DEFAULT_SE=$DPM_HOST
VO_DTEAM_STORAGE_DIR=$STORAGE_PATH/dteam
#VO_DTEAM_VOMS_SERVERS="'vomss://lcg-voms.cern.ch:8443/voms/dteam?/dteam' 'vomss://voms.cern.ch:8443/voms/dteam?/dteam'"
#VO_DTEAM_VOMSES="'dteam lcg-voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch dteam' 'dteam voms.cern.ch 15004 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch dteam'"
#VO_DTEAM_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"

# ESR
VO_ESR_SW_DIR=$VO_SW_DIR/esr
VO_ESR_DEFAULT_SE=$DPM_HOST
VO_ESR_STORAGE_DIR=$STORAGE_PATH/esr
VO_ESR_VOMS_SERVERS="'vomss://voms.grid.sara.nl:8443/voms/esr?/esr' "
VO_ESR_VOMSES="'esr voms.grid.sara.nl 30001 /O=dutchgrid/O=hosts/OU=sara.nl/CN=voms.grid.sara.nl esr' "
VO_ESR_VOMS_CA_DN="'/C=NL/O=NIKHEF/CN=NIKHEF medium-security certification auth' "

# GEANT4
VO_GEANT4_SW_DIR=$VO_SW_DIR/geant4
VO_GEANT4_DEFAULT_SE=$DPM_HOST
VO_GEANT4_STORAGE_DIR=$STORAGE_PATH/geant4
VO_GEANT4_VOMS_SERVERS="'vomss://lcg-voms.cern.ch:8443/voms/geant4?/geant4' 'vomss://voms.cern.ch:8443/voms/geant4?/geant4' "
VO_GEANT4_VOMSES="'geant4 lcg-voms.cern.ch 15007 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch geant4' 'geant4 voms.cern.ch 15007 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch geant4' "
VO_GEANT4_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' "

# LHCB
VO_LHCB_SW_DIR=/cvmfs/lhcb.cern.ch
VO_LHCB_DEFAULT_SE=$DPM_HOST
VO_LHCB_STORAGE_DIR=$STORAGE_PATH/lhcb
VO_LHCB_VOMS_SERVERS="'vomss://lcg-voms.cern.ch:8443/voms/lhcb?/lhcb' 'vomss://voms.cern.ch:8443/voms/lhcb?/lhcb' "
VO_LHCB_VOMS_EXTRA_MAPS="lcgprod lhcbprod"
VO_LHCB_VOMSES="'lhcb lcg-voms.cern.ch 15003 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch lhcb' 'lhcb voms.cern.ch 15003 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch lhcb' "
VO_LHCB_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' "


# MAGIC
VO_MAGIC_SW_DIR=$VO_SW_DIR/magic
VO_MAGIC_DEFAULT_SE=$DPM_HOST
VO_MAGIC_STORAGE_DIR=$STORAGE_PATH/magic
VO_MAGIC_VOMS_SERVERS="'vomss://voms01.pic.es:8443/voms/magic?/magic' 'vomss://voms02.pic.es:8443/voms/magic?/magic' "
VO_MAGIC_VOMSES="'magic voms01.pic.es 15003 /DC=es/DC=irisgrid/O=pic/CN=voms01.pic.es magic' 'magic voms02.pic.es 15003 /DC=es/DC=irisgrid/O=pic/CN=voms02.pic.es magic' "
VO_MAGIC_VOMS_CA_DN="'/DC=es/DC=irisgrid/CN=IRISGridCA' '/DC=es/DC=irisgrid/CN=IRISGridCA' "

# PLANCK
VO_PLANCK_SW_DIR=$VO_SW_DIR/planck
VO_PLANCK_DEFAULT_SE=$DPM_HOST
VO_PLANCK_STORAGE_DIR=$STORAGE_PATH/planck
VO_PLANCK_VOMS_SERVERS="'vomss://voms.cnaf.infn.it:8443/voms/planck?/planck' "
VO_PLANCK_VOMSES="'planck voms.cnaf.infn.it 15002 /C=IT/O=INFN/OU=Host/L=CNAF/CN=voms.cnaf.infn.it planck' "
VO_PLANCK_VOMS_CA_DN="'/C=IT/O=INFN/CN=INFN CA' "

# OPS
VO_OPS_SW_DIR=$VO_SW_DIR/ops
VO_OPS_DEFAULT_SE=$DPM_HOST
VO_OPS_STORAGE_DIR=$STORAGE_PATH/ops

#VO_OPS_VOMS_SERVERS="vomss://voms.cern.ch:8443/voms/ops?/ops/"
#VO_OPS_VOMSES="'ops voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch ops'"
#VO_OPS_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority'"

VO_OPS_VOMS_SERVERS="'vomss://lcg-voms.cern.ch:8443/voms/ops?/ops' 'vomss://voms.cern.ch:8443/voms/ops?/ops' "
VO_OPS_VOMSES="'ops lcg-voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch ops' 'ops voms.cern.ch 15009 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch ops' "
VO_OPS_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' '/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' "

# BABAR
VO_BABAR_SW_DIR=$VO_SW_DIR/babar
VO_BABAR_DEFAULT_SE=$DPM_HOST
VO_BABAR_STORAGE_DIR=$STORAGE_PATH/babar
VO_BABAR_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/babar?/babar' "
VO_BABAR_VOMSES="'babar voms.gridpp.ac.uk 15002 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk babar' "
VO_BABAR_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "

# CDF
VO_CDF_SW_DIR=$VO_SW_DIR/cdf
VO_CDF_DEFAULT_SE=$DPM_HOST
VO_CDF_STORAGE_DIR=$STORAGE_PATH/cdf
VO_CDF_VOMS_SERVERS="'vomss://voms.cnaf.infn.it:8443/voms/cdf?/cdf' 'vomss://voms-01.pd.infn.it:8443/voms/cdf?/cdf' 'vomss://voms.fnal.gov:8443/voms/cdf?/cdf' "
VO_CDF_VOMS_POOL_PATH=""
VO_CDF_VOMS_EXTRA_MAPS=""
VO_CDF_VOMSES="'cdf voms.cnaf.infn.it 15001 /C=IT/O=INFN/OU=Host/L=CNAF/CN=voms.cnaf.infn.it cdf' 'cdf voms-01.pd.infn.it 15001 /C=IT/O=INFN/OU=Host/L=Padova/CN=voms-01.pd.infn.it cdf' 'cdf voms.fnal.gov 15020 /DC=org/DC=doegrids/OU=Services/CN=http/voms.fnal.gov cdf' "
VO_CDF_VOMS_CA_DN="'/C=IT/O=INFN/CN=INFN CA' '/C=IT/O=INFN/CN=INFN CA' '/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1' "

# DZERO
VO_DZERO_SW_DIR=$VO_SW_DIR/dzero
VO_DZERO_DEFAULT_SE=$DPM_HOST
VO_DZERO_STORAGE_DIR=$STORAGE_PATH/dzero
VO_DZERO_VOMS_SERVERS="'vomss://voms.fnal.gov:8443/voms/dzero?/dzero' "
VO_DZERO_VOMS_POOL_PATH=""
VO_DZERO_VOMS_EXTRA_MAPS=""
VO_DZERO_VOMSES="'dzero voms.fnal.gov 15002 /DC=org/DC=doegrids/OU=Services/CN=http/voms.fnal.gov dzero' "

VO_DZERO_VOMS_CA_DN="'/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1' "

# HONE
VO_HONE_SW_DIR=$VO_SW_DIR/hone
VO_HONE_DEFAULT_SE=$DPM_HOST
VO_HONE_STORAGE_DIR=$STORAGE_PATH/hone
VO_HONE_VOMS_SERVERS="'vomss://grid-voms.desy.de:8443/voms/hone?/hone' "
VO_HONE_VOMSES="'hone grid-voms.desy.de 15106 /C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de hone' "
VO_HONE_VOMS_CA_DN="'/C=DE/O=GermanGrid/CN=GridKa-CA' "

# ILC
VO_ILC_SW_DIR=$VO_SW_DIR/ilc
VO_ILC_DEFAULT_SE=$DPM_HOST
VO_ILC_STORAGE_DIR=$STORAGE_PATH/ilc
VO_ILC_VOMS_SERVERS="'vomss://grid-voms.desy.de:8443/voms/ilc?/ilc' 'vomss://voms.fnal.gov:8443/voms/ilc?/ilc' "
VO_ILC_VOMSES="'ilc grid-voms.desy.de 15110 /C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de ilc' 'ilc voms.fnal.gov 15023 /DC=org/DC=doegrids/OU=Services/CN=http/voms.fnal.gov ilc' "
VO_ILC_VOMS_CA_DN="'/C=DE/O=GermanGrid/CN=GridKa-CA' '/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1' "


# NA48
VO_NA48_SW_DIR=$VO_SW_DIR/na48
VO_NA48_DEFAULT_SE=$DPM_HOST
VO_NA48_STORAGE_DIR=$STORAGE_PATH/na48
VO_NA48_VOMS_SERVERS="'vomss://voms.cern.ch:8443/voms/na48?/na48' "
VO_NA48_VOMS_EXTRA_MAPS="Role=admin na48adm"
VO_NA48_VOMSES="'na48 voms.cern.ch 15011 /DC=ch/DC=cern/OU=computers/CN=voms.cern.ch na48' "
VO_NA48_VOMS_CA_DN="'/DC=ch/DC=cern/CN=CERN Trusted Certification Authority' "

# ZEUS
VO_ZEUS_SW_DIR=$VO_SW_DIR/zeus
VO_ZEUS_DEFAULT_SE=$DPM_HOST
VO_ZEUS_STORAGE_DIR=$STORAGE_PATH/zeus
VO_ZEUS_VOMS_SERVERS="'vomss://grid-voms.desy.de:8443/voms/zeus?/zeus' "
VO_ZEUS_VOMS_POOL_PATH=""
VO_ZEUS_VOMS_EXTRA_MAPS=""
VO_ZEUS_VOMSES="'zeus grid-voms.desy.de 15112 /C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de zeus' "
VO_ZEUS_VOMS_CA_DN="'/C=DE/O=GermanGrid/CN=GridKa-CA' "

# GRIDPP
VO_GRIDPP_SW_DIR=$VO_SW_DIR/gridpp
VO_GRIDPP_DEFAULT_SE=$DPM_HOST
VO_GRIDPP_STORAGE_DIR=$STORAGE_PATH/gridpp
VO_GRIDPP_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/gridpp?/gridpp' "
VO_GRIDPP_VOMSES="'gridpp voms.gridpp.ac.uk 15000 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk gridpp' "
VO_GRIDPP_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "

# PHENO
VO_PHENO_SW_DIR=$VO_SW_DIR/pheno
VO_PHENO_DEFAULT_SE=$DPM_HOST
VO_PHENO_STORAGE_DIR=$STORAGE_PATH/pheno
VO_PHENO_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/pheno?/pheno' "
VO_PHENO_VOMSES="'pheno voms.gridpp.ac.uk 15011 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk pheno' "
VO_PHENO_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "

# CEDAR
VO_CEDAR_SW_DIR=$VO_SW_DIR/cedar
VO_CEDAR_DEFAULT_SE=$DPM_HOST
VO_CEDAR_STORAGE_DIR=$STORAGE_PATH/cedar
VO_CEDAR_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/cedar?/cedar' "
VO_CEDAR_VOMSES="'cedar voms.gridpp.ac.uk 15006 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk cedar' "
VO_CEDAR_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "

# MICE
VO_MICE_RBS="gfe01.hep.ph.ic.ac.uk lcgrb01.gridpp.rl.ac.uk lcgrb02.gridpp.rl.ac.uk"
VO_MICE_SW_DIR=$VO_SW_DIR/mice
VO_MICE_DEFAULT_SE=$DPM_HOST
VO_MICE_STORAGE_DIR=$STORAGE_PATH/mice
VO_MICE_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/mice?/mice' "
VO_MICE_VOMSES="'mice voms.gridpp.ac.uk 15001 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk mice' "
VO_MICE_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "

# LTWO
VO_LTWO_SW_DIR=$VO_SW_DIR/ltwo
VO_LTWO_DEFAULT_SE=$DPM_HOST
VO_LTWO_STORAGE_DIR=$STORAGE_PATH/ltwo
VO_LTWO_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/ltwo?/ltwo' "
VO_LTWO_VOMSES="'ltwo voms.gridpp.ac.uk 15005 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk ltwo' "
VO_LTWO_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "

# TOTALEP
VO_TOTALEP_SW_DIR=$VO_SW_DIR/totalep
VO_TOTALEP_DEFAULT_SE=$DPM_HOST
VO_TOTALEP_STORAGE_DIR=$STORAGE_PATH/totalep
VO_TOTALEP_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/totalep?/totalep' "
VO_TOTALEP_VOMSES="'totalep voms.gridpp.ac.uk 15026 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk totalep' "
VO_TOTALEP_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "

# FUSION
VO_FUSION_SW_DIR=$VO_SW_DIR/fusion
VO_FUSION_DEFAULT_SE=$DPM_HOST
VO_FUSION_STORAGE_DIR=$STORAGE_PATH/fusion
VO_FUSION_VOMS_CA_DN="'/DC=es/DC=irisgrid/CN=IRISGridCA' "
VO_FUSION_VOMS_SERVERS="'vomss://voms-prg.bifi.unizar.es:8443/voms/fusion?/fusion' "
VO_FUSION_VOMSES="'fusion voms-prg.bifi.unizar.es 15001 /DC=es/DC=irisgrid/O=bifi-unizar/CN=voms-prg.bifi.unizar.es fusion' "

# CALICE
VO_CALICE_SW_DIR=$VO_SW_DIR/calice
VO_CALICE_DEFAULT_SE=$DPM_HOST
VO_CALICE_STORAGE_DIR=$STORAGE_PATH/calice
VO_CALICE_VOMS_SERVERS="'vomss://grid-voms.desy.de:8443/voms/calice?/calice' "
VO_CALICE_VOMSES="'calice grid-voms.desy.de 15102 /C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de calice' "
VO_CALICE_VOMS_CA_DN="'/C=DE/O=GermanGrid/CN=GridKa-CA' "

# CAMONT
VO_CAMONT_SW_DIR=$VO_SW_DIR/camont
VO_CAMONT_DEFAULT_SE=$DPM_HOST
VO_CAMONT_STORAGE_DIR=$STORAGE_PATH/camont
VO_CAMONT_VOMS_CA_DN="'/C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA 2B' "
VO_CAMONT_VOMSES="'camont voms.gridpp.ac.uk 15025 /C=UK/O=eScience/OU=Manchester/L=HEP/CN=voms.gridpp.ac.uk camont' "
VO_CAMONT_VOMS_SERVERS="'vomss://voms.gridpp.ac.uk:8443/voms/camont?/camont' "

# MINOS
# see vo.d/minos.vo.gridpp.ac.uk
# SIXT
# see vo.d/vo.sixt.cern.ch

# SUPERNEMO
# see vo.d/supernemo.vo.eu-egee.org

# NGS
# see vo.d/ngs.ac.uk

# NORTHGRID
# see vo.d/vo.northgrid.ac.uk


